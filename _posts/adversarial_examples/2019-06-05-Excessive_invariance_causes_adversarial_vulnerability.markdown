---
layout: post
title:  "Excessive Invariance Causes Adversarial Vulnerability"
date:   2019-05-06 10:09:00 +0200
tags: [adversarial examples, reversible networks, iclr, 2019]
categories:  [Adversarial Examples]
author: Jacobsen et al, ICLR 2019, <a href='https://arxiv.org/pdf/1811.00401.pdf' target='_blank'>[link]</a>
thumb: /images/thumbs/eicav.png
year: 2019
---



<div class="summary">
The authors introduce the notion of <b>invariance-based adversarial examples</b>, and study them in the context of invertible networks, e.g. <code>iRevNet</code> <span class="citations">[1]</span>.
<ul>
<li><span class="procons">Pros (+):</span> Novel approach to adversarial examples, nice use of reversible networks.</li>
<li><span class="procons">Cons (-):</span> The analysis seems to mostly hold for fully reversible networks, which are not very common, and it is not clear if it provides insights about normal convnets.</li>
</ul>
</div>


<h3 class="section theory"> Definition: Invariance-based adversarial examples</h3>
Let $$F = D \circ  f_N \circ f_1$$ be a classification neural network, where $$f_{N}$$ corresponds to the output logits layer, and $$D: z \in \mathbb R^c \mapsto \arg\max_i \mbox{softmax}(z)_i$$ is the classifier decision. Let $$o$$ be an oracle $$o: \mathcal X \rightarrow \{1 \dots C\}$$.

* **Perturbation-based adversarial example:** We say $$x^{\ast}$$ is an *$$\epsilon$$-perturbation-based adversarial example* for $$x$$ if 
  * **(i)** $$x^{\ast}$$ is generated by an adversary and $$\|x - x^{\ast}\| <\epsilon$$
  * **(ii)** The perturbation modifies the output of the classifier: $$o(x^{\ast}) \neq F(x^\ast)$$.


* **Invariance-based adversarial example:** We say $$x^{\ast}$$ is an *invariance-based adversarial example* for $$x$$ at level $$i$$ if
  * **(i)** $$x$$ and $$x^{\ast}$$ have the same representations at level $$i$$, i.e., $$f_i \circ \dots f_1 (x) = f_i \circ \dots f_1 (x^{\ast}) $$ .
  * **(ii)** The perturbation modifies the output of the classifier: $$o(x^{\ast}) \neq o(x)$$

For instance, if we consider the oracle $$o$$ to be a human observer, then *perturbation-based adversarial examples* corresponds to cases where the classifier fails to correctly classify $$x^{\ast}$$, but to the human eye  $$x^{\ast}$$ is almost identical to $$x$$.

Orthogonal to this, *invariance-based adversarial examples* correspond to cases where $$x^{\ast}$$ is clearly distinct from $$x$$ and yet has the same activation responses under the network $$F$$: In other words, there is a mismatch between the invariances learned by the model (in the feature representations) and the actual semantic content of the input domain (defined by the oracle).

---

<h3 class="section proposed"> Analyzing Excessive Invariance with Reversible Networks</h3>

The authors consider a slightly modified version of the *i*-RevNet architecture <span class="citations">[1]</span>: $$f_N \circ \dots f_1$$ is now a *fully reversible function*, hence all the information is available at each layer, and no "nuisance feature" gets discarded as it is the case in standard architectures.

The classifier $$D$$ is built such that it only sees the first $$C$$ first  components from the logits output by $$f_N$$. These components are called *semantic variables* while the remaining ones are the *nuisance variables*.
To analyze the model, given semantic variables $$z_s: f_N \circ \dots f_1 (x)_{1 \dots c}$$ extracted from a given input $$x$$,  they sample nuisance variables $$\tilde z_n$$ and observe the reconstructed input $$f_1^{-1} \circ \dots f_N^{-1} (z_s | z_n)$$, where $$|$$ denotes concatenation. The authors call this process *metameric sampling*. While the samples are not guaranteed to be from the original distribution (as $$z_s$$ and $$z_n$$ can  not necessarily independently be sampled), in practice the resulting samples do look like they come from the original distribution.


#### Experiment 1: Synthetic Spheres Dataset
This is a binary classification task in $$\mathbb R^{500}$$, where a  point is classified to belong to one of two spheres. They perform two experiments: ***(i)*** Take two random points $$x_1$$ and $$x_2$$ and observe the decision boundary on the 2D subspace spanned by these two points and ***(ii)*** the same experiment but taking a sample $$x$$ and corresponding metameric sample $$\tilde{x}$$. This is done for the standard classifier $$D$$, predicting from the semantic variables, and for another classifier trained to predict from nuisance variables only.

In particular, the third picture from the left shows that one gets a very large *"adversarial direction"* where samples are moved from one sphere to the other, but the semantic classifier decisions remains the same. Note that this problem does not appear in the nuisance classifier (last picture), which indicates it captures additional task-relevant information discarded by the semantic classifier.


<div class="figure">
<img src="{{ site.baseurl }}/images/posts/excessive_invariance_1.png">
<p><b>Figure 1:</b>  Decision boundaries in 2D subspace spanned by two random data points (<b>left</b>) and boundaries  spanned by random datapoint and its metamer (<b>right</b>).</p>
</div>
 
 
#### Experiment 2: Natural Images
The experiments are done with the `MNIST` and `ImageNet` dataset. Visualizing metameric samples show that sampling from $$z_n$$ can significantly alter the image semantic content, while not modifying the actual semantic variables, $$z_s$$. 


<div class="figure">
<img src="{{ site.baseurl }}/images/posts/excessive_invariance_2.png">
<p><b>Figure 1:</b>  Top row are source images from
which we sample the logits, middle row are logit metamers and bottom row images from which
we sample the nuisances. Top row and middle row have the same (approximately for ResNets,
exactly for fully invertible RevNets) logit activations. Thus, it is possible to change the image
content completely without changing the 10- and 1000-dimensional logit vectors respectively. This
highlights a striking failure of classifiers to capture all task-dependent variability.</p>
</div>

----

<h3 class="section followup"> Information Theoretical Analysis and How to Defend Invariance-based Adversarial Examples </h3>

Let $$(x, y) \sim \mathcal D$$ with labels $$y \in [0, 1]^C$$. An ideal semantic classifier would be such that the *mutual information between semantic features and ground-truth labels*, $$I(z_s, y)$$ is maximized. 
However in the current scenario, we can incorporate two additional objectives aiming to disentangle the noise and semantic variables:

 * ***(i)*** Decreasing the mutual information $$I(z_n, y)$$ to reduce dependency between the ground-truth labels and nuisance features. The authors derive an objective based on a variational lower bound for the mutual information described in <span class="citations">[2]</span>.
 
 * ***(ii)*** Reducing the mutual information $$I(z_s, z_n)$$ which can be seen as a form of disentanglement between semantic and nuisance variables. This is incorporated in the training objective as a Maximum Likelihood Objective under a factorial prior, i.e., a prior which can be factorized $$p(z_s; z_n) - p(z_s) p(z_n)$$ which further encourages independence.

**Note on Theorem 8 (?)**: The two conditions $$I_{\mathcal D(z_n; y)} = 0$$ and $$I_{\mathcal D_{adv}(z_n; y)} \leq I_{\mathcal D(z_n; y)} $$ either contradict the non-negativity of mutual information or impose that $$I_{\mathcal D_{adv}(z_n; y)} = 0$$ which is a strong constraint.

Finally, experiments show that the newly proposed loss does encourage independence between the semantic and nuisance variables, although results are only available for the `MNIST` dataset.

---

<h3 class="section references"> References </h3>
* <span class="citations">[1]</span> i-RevNet: Deep Invertible Networks, <i>Jacobsen et al., ICLR 2018</i>
* <span class="citations">[2]</span> <i>The IM algorithm: A variational approach to information maximization, Barber and Agakov, NIPS 2003
