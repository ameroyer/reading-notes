<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Measuring abstract reasoning in neural networks | Your awesome title</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Measuring abstract reasoning in neural networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="⚫ ⚫ ⚫ ⚪ ⚪ Measuring abstract reasoning in neural networks Barrett et al., ICML 2018 [link]" />
<meta property="og:description" content="⚫ ⚫ ⚫ ⚪ ⚪ Measuring abstract reasoning in neural networks Barrett et al., ICML 2018 [link]" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-26T14:59:24+02:00" />
<script type="application/ld+json">
{"description":"⚫ ⚫ ⚫ ⚪ ⚪ Measuring abstract reasoning in neural networks Barrett et al., ICML 2018 [link]","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html","headline":"Measuring abstract reasoning in neural networks","dateModified":"2019-04-26T14:59:24+02:00","datePublished":"2019-04-26T14:59:24+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<body>  
<header class="site-header">

  <div class="wrapper">
  <li><a name="Few-Shot Learning">Few-Shot Learning</a>
    <ul>
      
      <li><a href="/reading_notes/2019/04/26/laso_label_set_operations_networks_for_multi_label_few_shot_learning.html">LaSO: Label-Set Operations networks for multi-label few-shot learning</a></li>
      
    </ul>
  </li>
  
  <li><a name="Visual Reasoning">Visual Reasoning</a>
    <ul>
      
      <li><a href="/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html">Measuring abstract reasoning in neural networks</a></li>
      
    </ul>
  </li>
  <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>

<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Measuring abstract reasoning in neural networks</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-04-26T14:59:24+02:00" itemprop="datePublished">Apr 26, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="-----measuring-abstract-reasoning-in-neural-networks">⚫ ⚫ ⚫ ⚪ ⚪ Measuring abstract reasoning in neural networks</h2>
<h4 id="barrett-et-al-icml-2018-link">Barrett et al., ICML 2018 <a href="https://arxiv.org/pdf/1807.04225.pdf">[link]</a></h4>

<h3 id="summary">Summary</h3>

<p>The authors introduce a new visual analogy dataset with the aim to analyze the reasoning abilities of ConvNets on higher abstract reasoning tasks such as small IQ tests.</p>
<ul>
  <li><strong>Pros (+):</strong> Introduces a new dataset for abstract reasoning and different evaluation procedures, considers a large range of baselines.</li>
  <li><strong>Cons(+):</strong> The Relation Network considers only pairwise interactions which is simple yet might be too specific to the problem at hand. Also hard to interpret and measure actual reasoning of the network.</li>
</ul>

<hr />

<h3 id="dataset">Dataset</h3>

<p>This paper introduces  the <em>Procedurally Generated Matrices</em> (PGM) dataset. It is based on <strong><em>Raven’s Progressive Matrices (RPM)</em></strong> introduced by psychologist John Raven in 1936. Given an incomplete <strong><em>3x3</em></strong> matrix (missing the bottom right panel), the goal is to complete the matrix with an image picked <strong><em>out of 8 candidates</em></strong>. Typically, several candidates are plausible but the subject has to select the one with the strongest justification.</p>

<center><img src="https://drive.google.com/uc?export=view&amp;id=1Nd7QA5574NeXmSimxhD1b4THfbYnXy_E" /></center>
<p><br /></p>

<h4 id="construction">Construction</h4>

<p>A PGM is defined as a set of triples $(r, o, a)$, each encoding a particular relation. For instance (<code class="highlighter-rouge">progression</code>, <code class="highlighter-rouge">lines</code>, <code class="highlighter-rouge">number</code>) means that the PGM contains a progression relation on the number of lines. In practice, the PGM dataset only contains <strong><em>1 to 4</em></strong> relations per PGM. The construction primitives are as follows:</p>

<ul>
  <li><strong>relation types</strong> ($R$, with elements $r$): <code class="highlighter-rouge">progression</code>, <code class="highlighter-rouge">XOR</code>, <code class="highlighter-rouge">OR</code>, <code class="highlighter-rouge">AND</code>, <code class="highlighter-rouge">consistent union</code>. The only relation that might require beyong binary correspondencies is the consistent union.</li>
  <li><strong>object types</strong> ($O$, with elements $o$): <code class="highlighter-rouge">shape</code> or <code class="highlighter-rouge">lines</code>.</li>
  <li><strong>attribute types</strong> ($A$, with elements $a$): <code class="highlighter-rouge">size</code>, <code class="highlighter-rouge">type</code>, <code class="highlighter-rouge">colour</code>, <code class="highlighter-rouge">position</code>, <code class="highlighter-rouge">number</code>. Each attribute takes values in a discrete set (e.g. 10 levels of gray intensity for colour).</li>
</ul>

<p>Note that some relations are hard to define (for instance progresson on shape position ?), and hence ignored. In total, <strong><em>29</em></strong> possible relations triples are considered.</p>

<p>The attributes which are not involved in any of the relations of the PGM are called the <strong><em>nuisance attributes</em></strong>. They are chosen either as a fixed value for all images in the squence, or randomly assigned (<strong><em>distracting setting</em></strong>).</p>

<h4 id="evaluation-setting">Evaluation Setting</h4>
<p>The authors consider 8 generalization settings to evaluate on:</p>

<ul>
  <li>
    <p><strong>Neutral setting.</strong> Standard random train/test split, no constraint on the relations</p>
  </li>
  <li>
    <p><strong>Interpolation</strong> and <strong>Extrapolation.</strong> The values of the <code class="highlighter-rouge">colour</code> and <code class="highlighter-rouge">size</code> attributes are restricted to half the possible values in the training set, and take values in the remaining half options in the test set. Note that in this setting, the test set is built such that every sequence contains one of these two attributes, i.e. generalization is required for every image. The different between inter- and extrapolation lies in the <strong><em>discretized space split</em></strong>: For interpolation, the split is uniform across the support (even-indexed values vs. odd-indexed values). In extrapolation, the values are split between lower half of the space and upper half of the space.</p>
  </li>
  <li><strong>Held-out setting.</strong> As the name indicates, this evaluation setting consists in keeping certain relations out of the training set and considering them only at test time (each of the test question contains at least one of the kept-out relations).</li>
  <li><em>shape-colour.</em> Keep out any relation with $o=$ <code class="highlighter-rouge">shape</code> and $a =$ <code class="highlighter-rouge">colour</code></li>
  <li><em>line-type.</em> Keep out any relation with $o=$ <code class="highlighter-rouge">line</code> and $a =$ <code class="highlighter-rouge">type</code></li>
  <li><em>triples</em>. Take out <strong>seven</strong> relation triples (chosen such that every attribute is represented exactly once (<strong>?: but there’s only five attributes</strong>)).</li>
  <li><em>pairs of triples</em>. Same as before but considering pairs of triples this time and only generating PGM with at least two relations: in that way, some relation interactions will have never been seen on training time.</li>
  <li><em>pairs of attributes</em>. Same as before but at the attribute level</li>
</ul>

<hr />

<h3 id="baselines">Baselines</h3>
<p>The main contributions of the paper are to introduce the PGM dataset and evaluate several standard deep architectures on it:</p>

<ul>
  <li>
    <p><strong>CNN-MLP:</strong> A standard 4-layers CNN, followed by 2 fully connected layers. It takes as inputs the 8 context panels of the matrix and the 8 panel candidates concatenated on the channel axis: i.e., inputs to the model are 80x80x16 images. It outputs the labels of the correct panels (8-labels classification task).</p>
  </li>
  <li>
    <p><strong>ResNet.</strong> Same as before but with a ResNet architecture.</p>
  </li>
  <li>
    <p><strong>Wild resNet.</strong> This time, the candidate panels are fed separately (i.e. 8 different input, each as a 9 channel image) and a score is output for each one of them. The candidate with the higest score is chosen.</p>
  </li>
  <li>
    <p><strong>Context-blind ResNet.</strong> Rather a “sanity check” than a baseline, train a ResNet that only takes the candidate panels as inputs, no context.</p>
  </li>
  <li>
    <p><strong>LSTM.</strong> First, each of the 16 panels is fed independently through a 4-layers CNN and the output feature maps is tagged with an index (following the sequence order). This sequence is fed through a LSTM, whose final hidden state is passed through one linear layer for the final classification.</p>
  </li>
  <li>
    <p><strong>RN network.</strong> The authors propose a Relation Network based on recent work  [1]. Each context panel and candidate is fed through a CNN resulting in embeddings ${x_1 \dots x_8}$ and  ${c_1 \dots c_8}$ respectively. Then for each candidate panel $k$, the Relation Network outputs a score $s_k$:</p>
  </li>
</ul>

<p>\begin{align}
s_k = f_{\phi} \left( \sum_{x, y \in {x_1 \dots x_8, c_k}^2 } g_{\theta}(x, y) \right)
\end{align}</p>

<p>Additionally, they consider a semi-supervised variant where the model tries to additionally predict the relations  underlying the PGM (encoded as a one-hot vector) as a <strong><em>meta-target</em></strong>. The total loss is a weighted average between the candidate classification loss term and the meta-target regression loss term.</p>

<hr />

<h3 id="experiments">Experiments</h3>

<h4 id="overall-results">Overall results</h4>

<p>The CNN-based  models perform consistantly badly, while LSTM provides an improvement but a small one. The Wild ResNet provides futher improvement over ResNet, which shows that using a panel scoring structure is more beneficial than direct classification of the correct candidate. Finally <strong><em>WReN outperforms all other baselines</em></strong>, which could be expected as it makes use of pairwise interactions across panels. The main benefit of the method is its simplicity (<em>Note:</em> it could be interesting to compare agains other sequential architecture on ResNet).</p>

<h3 id="different-evaluation-procedure">Different evaluation procedure</h3>

<p>While the WReN achives satisfying accuracy on the <strong>neutral</strong> and <strong>interpolation</strong> splits (~ 60%), as one would expect this does not hold for the more challenging settings, e.g. it significantly drops to 17% on the <strong><em>extrapolation</em></strong> setting.</p>

<p>More generally, it seems that the model <strong><em>has troubles generalizing</em></strong> when some attributes are never seen during the training, (e.g., extrapolation or attr.rels settings) which seems to indicate the model probably more easily picks visual properties rather than  high-level abstract reasoning ones.</p>

<h4 id="detailed-results">Detailed results</h4>
<p>The authors also report results broken down by number of relations per matrix, relation types and ttribute types(when only one relation). As one would expect, one-relation are the easiest to solve, but, interestingly, it is slightly easier to solve three-relations matrices than four-relations one, which might be because it determines a more precise answer.</p>

<p>As for relations,<code class="highlighter-rouge">XOR</code> and <code class="highlighter-rouge">progression</code> are the hardest to solve although the model still performs decently well on those (50%).</p>

<hr />

<h3 id="closely-related-follow-up-work">Closely related (follow-up work)</h3>

<h4 id="1-improving-generalization-for-abstract-reasoning-tasks-using-disentangled-feature-representations">1. Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations.</h4>
<p><strong>Steenbrugge et al., <a href="https://arxiv.org/abs/1811.04784">[link]</a></strong></p>

<blockquote>
  <p>The main observation is that the previously proposed model seems to <strong><em>disregard high-level abstract relations</em></strong> (e.g. considering the poor accuracy on the extrapolation set). this paper proposes to improve the encoding step by embedding the panel in a <strong>“disentangled”</strong> space using a $\beta$-VAE. (Note the apostrophes around <em>disentangled</em>, as, as far as I know, there is no guarantee of this property for $\beta$-VAE;  it has only been studied experimentally on specific datasets e.g. CelebA).</p>
</blockquote>

<blockquote>
  <p>There are also a few weird details in the experimental section. For instance, they claim the RN embedding has dimension 512, while it has dimension 256 (it only becomes 512 whe concatenating in $g_{\theta}$). Second, their own VAE embedding has latent dimension 64 and it’s not clear why they wouldn’t use more dimensions fo a fairer comparison (my guess is it might lose the “disentangled” property). In the end their encoder adds two additional fully-connected layers for no apparent good reasons.</p>
</blockquote>

<blockquote>
  <p>The model yields some improvement, especially on the more challenging settings (roughly 5% at best). They however omit results in the extrapolation regime, for unkown reasons.</p>
</blockquote>

<hr />

<h3 id="references">References</h3>
<ul>
  <li>[1] A simple neural network module for relational reasoning, Santoro et al., NIPS 2017</li>
</ul>

  </div><a class="u-url" href="/jekyll/update/2019/04/26/welcome-to-jekyll-(copy).html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Wrfdfsdte an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
